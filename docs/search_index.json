[["index.html", "Holo-omics workbook", " Holo-omics workbook Holo-omics overview. Modified from Nyholm et al. 2020 [1] The Holo-omics workbook is a compilation of methodological procedures to generate, analyse and integrate holo-omic data, i.e., multi-omic data jointly generated from hosts and associated microbial communities [1, 2]. This resource extends the contents of the article “A practical introduction to holo-omics”, which aims at guiding researchers to the main critical steps and decision points to perform holo-omic studies. While the article focuses on discussing pros and cons of using multiple available options, the aim of the workbook is to compile protocols and pipelines to be implemented by researchers. The Holo-omics Workbook is presented in two formats: Website (https://holo-omics.github.io/) PDF document (https://holo-omics.github.io/holo-omics_workbook.pdf) These resources are presented as two of the main final outputs of the H2020 project HoloFood. More information about this EU Innovation Action that ran between 2019 and 2023 can be found in the [HoloFood section][HoloFood] in this workbook, the HoloFood Website and the CORDIS website. Contents Introduction: general information about holo-omics, employed data types and study design considerations. Laboratory procedures: methods and procedures for generating raw omic data of hosts and microbial communities. Bioinformatic procedures: methods and procedures for processing raw omic data into quantitative datasets to be analysed through statistics. Statistical procedures: methods and procedures for analysing and integrating holo-omic data. Protocols, exercises and tutorials The workbook contains example data and bits of code (mostly shell and R) to reproduce data generation and analysis procedures. Code boxes look like the following: shao4d_perm &lt;- shao4d %&gt;% tax_transform(&quot;identity&quot;, rank = &quot;genus&quot;) %&gt;% dist_calc(&quot;aitchison&quot;) %&gt;% dist_permanova( variables = c(&quot;birth_mode&quot;, &quot;sex&quot;, &quot;number_reads&quot;), n_perms = 99, # you should use more permutations in your real analyses! n_processes = 1 ) #&gt; Dropping samples with missings: 15 #&gt; 2022-11-24 01:15:20 - Starting PERMANOVA with 99 perms with 1 processes #&gt; 2022-11-24 01:15:21 - Finished PERMANOVA shao4d_perm %&gt;% perm_get() #&gt; Permutation test for adonis under reduced model #&gt; Marginal effects of terms #&gt; Permutation: free #&gt; Number of permutations: 99 #&gt; #&gt; vegan::adonis2(formula = formula, data = metadata, permutations = n_perms, by = by, parallel = parall) #&gt; Df SumOfSqs R2 F Pr(&gt;F) #&gt; birth_mode 1 10462 0.09055 29.3778 0.01 ** #&gt; sex 1 402 0.00348 1.1296 0.31 #&gt; number_reads 1 1117 0.00967 3.1364 0.01 ** #&gt; Residual 287 102209 0.88462 #&gt; Total 290 115540 1.00000 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Authors Antton Alberdi (*corresponding author), University of Copenhagen Morten T Limborg, University of Copenhagen Iñaki Odriozola, University of Copenhagen Jacob A Rasmussen, University of Copenhagen Protocol and script contributors Carlotta Pietroni, University of Copenhagen Raphael Eisenhofer, University of Copenhagen Jorge Langa, University of Copenhagen Other relevant people Tom Gilbert (HoloFood project coordinator), University of Copenhagen Anna Fotaki (HoloFood project manager), University of Copenhagen How to cite this work Instructions to cite this work will be eventually added. Acknowledgement This project has received funding from the European Unionʼs Horizon 2020 research and innovation programme under grant agreement No 817729. References "],["holo-omics.html", "1 Introduction to holo-omics", " 1 Introduction to holo-omics Holo-omics refers to the methodological approach that jointly generates and analyses multi-omic data from hosts and associated microbial communities [1]. The holo-omic approach to host-microbiota interactions relies on three major assumptions: Host-associated microorganisms interact not only with each other but also with their host [3]. These interactions affect, either positively or negatively, central biological processes of hosts and microorganisms [4]. The interplay can be traced using biomolecular tools. References "],["omic-layers.html", "1.1 Omic layers", " 1.1 Omic layers Nucleic acid sequencing and mass spectrometry technologies that enable tracking the biomolecular pathways linking host and mi- crobial genomic sequences with biomolecular phenotypes by generating (meta)transcriptomes, (meta) proteomes, and (meta)metabolomes. The same technologies also enable epigenomic and exposomic profiling, which can further contribute to disentangling the biochemical associations between host-micro- biota-environment interactions and their effect on host phenotypes In this workbook we consider seven omic layers that require specific data generation and analysis strategies before integrating them into multi-omic statistical models: Nucleic acid sequencing-based Host genomics - HG Host transcriptomics - HT Microbial metagenomics - MG Microbial metatranscriptomics - MT Mass spectrometry-based Host proteomics - HP Microbial metaproteomics - MP (Meta)metabolomics - ME Acknowledging the distinct biological and structural characteristics of these seven omic layers is essential to design experiments and analytical pipelines for better solving the complex puzzle of host-microbiota interactions. "],["host-genomics.html", "1.2 Host genomics (HG)", " 1.2 Host genomics (HG) Contents to be added "],["host-transcriptomics.html", "1.3 Host transcriptomics (HT)", " 1.3 Host transcriptomics (HT) Contents to be added "],["microbial-metagenomics.html", "1.4 Microbial metagenomics (MG)", " 1.4 Microbial metagenomics (MG) Contents to be added "],["microbial-metatranscriptomics.html", "1.5 Microbial metatranscriptomics (MT)", " 1.5 Microbial metatranscriptomics (MT) Contents to be added "],["host-proteomics.html", "1.6 Host proteomics (HP)", " 1.6 Host proteomics (HP) Contents to be added "],["microbial-metaproteomics.html", "1.7 Microbial metaproteomics (MP)", " 1.7 Microbial metaproteomics (MP) Contents to be added "],["meta-metabolomics.html", "1.8 (Meta)metabolomics (ME)", " 1.8 (Meta)metabolomics (ME) Contents to be added "],["study-design-considerations.html", "2 Study design considerations", " 2 Study design considerations The contents of this section have been extracted and modified from the article Disentangling host–microbiota complexity through hologenomics published in Nature Reviews Genetics in 2022 by the authors of the Holo-omics Workbook. Holo-omic approaches can be used to understand how the combined features of hosts and microorganisms shape biological processes relevant for hosts (such as adaptation), for microorganisms (such as meta-community dynamics) or both [5]. Depending on the aims and features of the study system, holo-omics can be implemented using different study designs, model systems and techniques. This landscape of possibilities is shaped around five essential questions that need to be considered when designing and interpreting hologenomic studies, which relate to five core topics: Hologenomic complexity Control of variables Molecular resolution Spatiotemporal factors Explanatory and response variables References "],["hologenomic-complexity.html", "2.1 Hologenomic complexity", " 2.1 Hologenomic complexity The contents of this section have been extracted and modified from the article Disentangling host–microbiota complexity through hologenomics published in Nature Reviews Genetics in 2022 by the authors of the Holo-omics Workbook. Hologenomic complexity can be broadly defined as the amount of information relevant to the study that the biological system under analysis contains and it can be decomposed into three major elements: host genomic, microbial metagenomic and environmental complexity [5]. Within each of these elements, two sources of complexity can be defined: the intrinsic complexity of the system under study, including host genome size and number of bacterial genomes, and the complexity introduced by the degree of difference between the organisms under comparison such as gene expression differences versus distinct genomes. Decomposition of hologenomic complexity. (a-c) The design and interpretation of hologenomic studies depend on the host genomic (part a), microbial metagenomic (part b) and environmental (part c) complexity of the system under study. Within each axis of complexity, two types of gradients can be defined based on whether the features are intrinsic to the system or introduced by the researcher through the selection of groups under comparison. (d) Six examples of study systems with different levels of genomic, metagenomic and environmental complexity. (e) Three-dimensional representation of the complexity of the examples. The area of the plain represents the combined host genomic and microbial metagenomic complexity of the system, while the height represents the environmental complexity. The combined three-dimensional volume represents the overall hologenomic complexity of the system. HMP: Human Microbiome Project. References "],["control-of-variables.html", "2.2 Control of variables", " 2.2 Control of variables The contents of this section have been extracted and modified from the article Disentangling host–microbiota complexity through hologenomics published in Nature Reviews Genetics in 2022 by the authors of the Holo-omics Workbook. Controlling the complexity of hologenomic variables is essential for addressing specific research questions. Broadly speaking, the more detailed and mechanistic the question under study, the greater the required control. For instance, research on specific biomolecular processes using laboratory models will require a higher level of control than studying biogeographical patterns of host–microbiota interactions in wild organisms. The control of hologenomic variables can be achieved through a number of strategies. 2.2.1 Controlling host genomes To be added. 2.2.2 Controlling microbial metagenomes To be added. 2.2.3 Controlling the environment To be added. "],["molecular-resolution.html", "2.3 Molecular resolution", " 2.3 Molecular resolution The contents of this section have been extracted and modified from the article Disentangling host–microbiota complexity through hologenomics published in Nature Reviews Genetics in 2022 by the authors of the Holo-omics Workbook. To be added. "],["spatiotemporal-factors.html", "2.4 Spatiotemporal factors", " 2.4 Spatiotemporal factors The contents of this section have been extracted and modified from the article Disentangling host–microbiota complexity through hologenomics published in Nature Reviews Genetics in 2022 by the authors of the Holo-omics Workbook. To be added. "],["explanatory-and-response-variables.html", "2.5 Explanatory and response variables", " 2.5 Explanatory and response variables The contents of this section have been extracted and modified from the article Disentangling host–microbiota complexity through hologenomics published in Nature Reviews Genetics in 2022 by the authors of the Holo-omics Workbook. To be added. "],["about-labwork.html", "3 About labwork", " 3 About labwork Laboratory protocols Nucleic-acid sequencing-based approaches DNA/RNA extraction for HG, HT, MG and MT Sequencing library preparation for HG and MG Sequencing library preparation for HT Sequencing library preparation for MT Mass spectrometry-based approaches Protein extraction for HP and MP Metabolite extraction for ME "],["dna-rna-extraction.html", "4 DNA/RNA extraction", " 4 DNA/RNA extraction Hundreds or (probably) thousands of different protocols and variations exist for extracting and purifying nucleic acids. Protocols can be classified based on methodological (e.g., chemical vs. physical DNA/RNA isolation, column-based vs bead-based, commercial vs. open-access). Sample preprocessing Bead-beating Contents to be added Freeze-heat shock Contents to be added Tissue digestion Contents to be added Chemical isolation Based on chemical separation of nucleic acids from the rest of molecules. Physicochemical isolation Based on physical separation of nucleic acids from the rest of molecules. Column-based Contents to be added Bead-based Contents to be added Available protocols Contents to be added "],["protein-metabolite-extraction.html", "5 Protein/metabolite extraction", " 5 Protein/metabolite extraction Laboratory protocols for protein/metabolite extraction "],["sequencing-library-preparation.html", "6 Sequencing library preparation", " 6 Sequencing library preparation Laboratory protocols "],["subheading.html", "6.1 Subheading", " 6.1 Subheading Test text "],["about-bioinformatics.html", "7 About bioinformatics", " 7 About bioinformatics Bioinformatic processing of raw sequencing and mass spectrometry data is the computational step that precedes statistical analyses and integration of multi-omic data. Through bioinformatic processing raw data are converted into meaningful bits of information, usually drastically decreasing the size of the data sets that are used for downstream analyses. Raw sequencing and mass spectrometry-based data files used in holo-omic analyses are typically in the realm of gigabytes (Gb) or even terabytes (Tb). Many of the performed operations require large amounts of memory (some more than 1Tb), which makes it impossible to process data in personal computers. Instead, most bioinformatics tasks are performed in computational clusters with access to large amounts of memory and many CPUs and GPUs, which enable parallelising computational tasks thus speeding up data processing time. However, for the sake of simplicity and practicality, the example datasets included in this Workbook have been considerably downscaled to enable reproducing the exercises in personal computers. All bioinformatic analyses included in the Holo-omics workbook are conducted in a Unix command line Shell environment (BASH/SH). You can find the details to set-up your SHELL environment in the section Prepare your Shell environment. "],["prepare-shell.html", "7.1 Prepare your shell environment", " 7.1 Prepare your shell environment If the comment chunks of the code (text after #) is creating you problems, use the following code to disable interactive comments and avoid issues when copy-pasting code: setopt interactivecomments Required software Bioinformatic pipelines for processing omic data require the use of dozens of software. All the required software are listed in the conda environment installation file available here. Install miniconda If conda is not installed in your system, the first step is to install miniconda (a free minimal installer for conda, enough to run the bioinformatic analyses explained in the workbook). Miniconda installers for Linux, Mac and Windows operating systems can be found in the following website: https://docs.conda.io/en/latest/miniconda.html Once conda or miniconda is installed in your system, you should be able to create and manage your conda environments. You can test whether conda has been succesfully installed using the following code: conda -V #&gt; conda 22.11.1 #or whatever version you have installed Install mamba (optional) An optional step is to install mamba, which is a reimplementation of the conda package manager in C++, which speeds up many of the processes. Mamba can be installed through the command line using the conda install option. conda install mamba -n base -c conda-forge Create a conda environment All the bioinformatic analyses explained in this workbook will be run within an environment containing all the necessary software. The file that specifies which software to install in the environment is available here, and can be retrieved using wget (as shown in the code below), or downloading from the Internet browser. If using the latter option, don’t forget to provide the absolut path to the ‘holo-omics-env.yaml’ file in the mamba create command. wget https://raw.githubusercontent.com/holo-omics/holo-omics.github.io/main/bin/holo-omics-env.yaml #download installer file conda update conda #to ensure everything is updated conda deactivate #deactivate any conda environment before creating a new one conda env create -f holo-omics-env.yml rm holo-omics-env.yaml #remove installer file As the environment contains dozens of softwares, the process of creating it will take a while. It is recommended to have a good Internet connection to speed-up software download. Once the installation is over, you can double-check whether the environment has been successfully created using the following script: conda activate holo-omics #&gt; (holo-omics) anttonalberdi@Anttons-MBP ~ % The (holo-omics) specifies the environment you are at. To get out of the environment use: conda env list #&gt; base * /Users/anttonalberdi/miniconda3 #&gt; holo-omics /Users/anttonalberdi/miniconda3/envs/holo-omics Activate the holo-omics conda environment Whenever running the holo-omic analyses explained in this workbook, it will be necessary to activate the holo-omics environment through the following command: conda activate holo-omics #&gt; (holo-omics) anttonalberdi@Anttons-MBP ~ % Install software in conda environment Content to be added. #Example code goes here "],["example-data-bioinformatics.html", "7.2 Example data for bioinformatics", " 7.2 Example data for bioinformatics Contents to be added here. #Example code goes here "],["using-snakemake.html", "7.3 Using snakemake for workflow management", " 7.3 Using snakemake for workflow management Contents to be added here. #Example code goes here "],["sequencing-data-preprocessing.html", "8 Sequencing data preprocessing", " 8 Sequencing data preprocessing The first step of the bioinformatic pipeline is to pre-process the raw sequencing data to prepare them for downstream analyses. Preprocess the reads using fastp Raw sequencing data require an initial preprocessing to get rid off low-quality nucleotides and reads, as well as any remains of sequencing adaptors that can mess around in the downstream analyses. An efficient way to do so is to use the software fastp, which can perform all above-mentioned operations in a single go and directly on compressed files. fastP documentation can be found here. fastp \\ --in1 {input.r1i} --in2 {input.r2i} \\ --out1 {output.r1o} --out2 {output.r2o} \\ --trim_poly_g \\ --trim_poly_x \\ --low_complexity_filter \\ --n_base_limit 5 \\ --qualified_quality_phred 20 \\ --length_required 60 \\ --thread {threads} \\ --html {output.fastp_html} \\ --json {output.fastp_json} \\ --adapter_sequence {params.adapter1} \\ --adapter_sequence_r2 {params.adapter2} Splitting host and non-host data Depending on the sample type employed for data generation, sequencing data might contain only host reads, only microbial reads, or a mixture of both. For example, blood sampled from an animal is expected to only contain host DNA/RNA reads (unless an infection is ongoing), while DNA extracted from a microbial culture is only expected to contain microbial DNA/RNA reads (unless human contamination has happened). In contrast, intestinal content samples, faecal samples, leave samples or root samples can contain both host and microbial nucleic acids. Index host genome bowtie2-build \\ --large-index \\ --threads {threads} \\ {output.rn_catted_ref} {output.rn_catted_ref} Map samples to host genomes # Map reads to catted reference using Bowtie2 bowtie2 \\ --time \\ --threads {threads} \\ -x {input.catted_ref} \\ -1 {input.r1i} \\ -2 {input.r2i} \\ | samtools view -b -@ {threads} - | samtools sort -@ {threads} -o {output.all_bam} - &amp;&amp; # Extract non-host reads (note we&#39;re not compressing for nonpareil) samtools view -b -f12 -@ {threads} {output.all_bam} \\ | samtools fastq -@ {threads} -1 {output.non_host_r1} -2 {output.non_host_r2} - &amp;&amp; # Send host reads to BAM samtools view -b -F12 -@ {threads} {output.all_bam} \\ | samtools sort -@ {threads} -o {output.host_bam} - "],["host-genomics-data-processing.html", "9 Host genomics (HG) data processing", " 9 Host genomics (HG) data processing Contents to be added. "],["host-reference-genome.html", "9.1 Host reference genome", " 9.1 Host reference genome Contents to be added. "],["host-genome-resequencing.html", "9.2 Host genome resequencing", " 9.2 Host genome resequencing Contents to be added. "],["microbial-metagenomics-data-processing.html", "10 Microbial metagenomics (MG) data processing", " 10 Microbial metagenomics (MG) data processing Microbial metagenomic data processing can be conducted following different strategies. Decision on which approach to use should be based on the aims of the study, available reference data, amount of generated data, and many other criteria. In this workbook we consider three main approaches that require different bioinformatic pipelines to be implemented. Reference-based approach: it relies on a reference database of microbial genomes to which sequencing reads can be mapped to obtain estimations of relative proportion of reads belonging to each of the genomes available in the reference database. It is the simplest and computationally less expensive approach, yet it completely relies on a complete and representative reference database. Assembly-based approach: it is based on assembling sequencing reads into longer DNA sequences known as contigs, which can then be used to predict genes and perform functional analyses. The main limitation of this approach is that the entire metagenome (set of contigs) in each sample is considered as a single unit, thus overlooking which bacterial genome each detected gene belongs to. Genome-resolved approach: it is the most advanced of the three approaches, and the strategy that provides the largest amount of information, as the aim of this approach is to directly reconstruct all the genomes in a metagenome. This is achieved by binning contigs into Metagenome-Assembled Genomes (MAGs), which can then be taxonomically and functionally annotated to perform sounds community-level analyses. "],["reference-based.html", "10.1 Reference-based", " 10.1 Reference-based Assembly-based. "],["assembly-based.html", "10.2 Assembly-based", " 10.2 Assembly-based Assembly-based approaches can be divided in two main strategies: Individual assembly-based Coassembly-based 10.2.1 Individual assembly-based Two of the most popular metagenome assemblers are Megahit and MetaSpades. Metaspades is considered superior in therms of assembly quality, yet memory requirements are much larger than those of Megahit. Thus, one of the most relevant criteria to choose the assembler to be employed is the balance between amount of data and available memory. Another minor, yet relevant difference between both assemblers is that Megahit allows removing contings below a certain size, while MetaSpades needs to be piped with another software (e.g. bbmap) to get rid off barely informative yet often abundant short contigs. 10.2.1.1 Individual assembly using Megahit megahit \\ -t {threads} \\ --verbose \\ --min-contig-len 1500 \\ -1 {input.r1} -2 {input.r2} \\ -o {params.workdir} 2&gt; {log} 10.2.1.2 Individual assembly using MetaSpades metaspades.py \\ -t {threads} \\ -k 21,33,55,77,99 \\ -1 {input.r1} -2 {input.r2} \\ -o {params.workdir} 2&gt; {log} # Remove contigs shorter than 1,500 bp using bbmap reformat.sh \\ in={params.workdir}/scaffolds.fasta \\ out={output.assembly} \\ minlength=1500 10.2.1.3 Get assembly statistics using Quast The metagenome assemblies can have very different properties depending on the amount of data used for the assembly, the complexity of the microbial community, and other biological and technical aspects. It is therefore convenient to obtain some general statistics of the assemblies to decide whether they look meaningful to continue with downstream analyses. This can be easily done using the software Quast. quast \\ -o {output.report} \\ --threads {threads} \\ {input.assembly} TO BE CONTINUED FROM HERE: https://github.com/earthhologenome/EHI_bioinformatics/blob/main/0_Code/2_Individual_Assembly_Binning.snakefile 10.2.2 Coassembly-based "],["genome-resolved.html", "10.3 Genome-resolved", " 10.3 Genome-resolved Contents to be added. "],["host-transcriptomics-data-processing.html", "11 Host transcriptomics (HT) data processing", " 11 Host transcriptomics (HT) data processing Contents to be added. "],["microbial-metatranscriptomics-data-processing.html", "12 Microbial metatranscriptomics (HT) data processing", " 12 Microbial metatranscriptomics (HT) data processing Contents to be added. "],["host-proteomics-data-processing.html", "13 Host proteomics (HP) data processing", " 13 Host proteomics (HP) data processing Contents to be added. "],["microbial-metaproteomics-data-processing.html", "14 Microbial metaproteomics (MP) data processing", " 14 Microbial metaproteomics (MP) data processing Contents to be added. "],["about-statistics.html", "15 About statistics", " 15 About statistics Statistics is probably the most challenging step of holo-omic studies, due to two main factors: the extreme complexity of the data, often containing thousands of features, and the limited sample size, often in the realm of the dozens of sampling units. This combination renders many holo-omic datasets rather statistics unfriendly. A step-by-step approach In this workbook we strongly encourage researchers to proceed step-by-step when dealing with holo-omics data and biological questions. Initial quantitative exploration of omic layers The analysis of any multi-omic data should begin with independent analysis of each omic layer to learn about its structure and variability before jumping to multi-omic data integration. Data transformations: multivariate datasets consist of different data types (e.g., presence-absence of taxa, counts of genes, community-level metabolic capacity index of a function, concentrations of metabolites across samples) that may require specific transformation before applying statistical techniques. Unsupervised exploration of omic layers: include exploratory techniques, such as cluster analysis and ordination-based visualisation methods, which reveal the structure and main patterns of the omic datasets without prior information about experimental design. These procedures might reveal that the observations are structured into meaningful groups or that variables can be reduced to fewer dimensions. Supervised analysis of omic layers: this type of analyses incorporate information of experimental design and aim at testing and estimating the effects of the experimental factors (e.g., dietary treatment, drug administration) or variables of interest (e.g., age of the experimental subjects, geographic location of studied populations) on different omic layers. Multi-omic data integration When it comes to multi-omic data integration, the approaches can be broadly categorised into two types: multi-staged analysis and meta-dimensional or simultaneous analysis. Multi-staged integration: leverages the central dogma of molecular biology to assume that the variation in omic datasets is hierarchical, such that variation in DNA leads to variation in RNA and so on to determine the phenotype Meta-dimensional integration: considers the possibility that the phenotype is the product of the combination of variation across all omic layers, with the presence of complex inter-omic interactions. All statistical analyses included in the Holo-omics workbook are conducted in R environment. You can find the details to set-up your R environment in the section Prepare your R environment. "],["prepare-r.html", "15.1 Prepare your R environment", " 15.1 Prepare your R environment All statistical analyses included in the Holo-omics workbook are conducted in R environment [6]. R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows and MacOS, and in order to use it, R or RStudio must be installed in your local computer or remote server. Required packages In order to reproduce the analyses shown in the workbook, a rather long list of R packages must be installed. Packages are the fundamental units of reproducible R code, which include reusable R functions, the documentation that describes how to use them, and sample data. ape DESeq2 distillR ggplot2 tidyverse vegan (…) Package installation Packages are installed programatically using three main ways: through CRAN, Bioconductor or Github. Install package from CRAN CRAN is a network of ftp and web servers around the world that store identical, up-to-date, versions of code and documentation for R. Packages stored in CRAN can be installed using the following code: install.packages(&quot;package_name&quot;) #e.g. install.packages(&quot;vegan&quot;) Install package from Bioconductor Bioconductor is a free, open source and open development software project for the analysis and comprehension of genomic data generated by wet lab experiments in molecular biology. Packages included in Bioconductor can be installed using the following code: if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;package_name&quot;) #e.g. BiocManager::install(&quot;DESeq2&quot;) Install package from Github GitHub is a code hosting platform for version control and collaboration. Packages stored in R can be installed using the following code after installing the package devtools: library(devtools) install_github(&quot;github_repository_name_of_the_package&quot;) #e.g. install_github(&quot;anttonalberdi/distillR&quot;) References "],["example-data-statistics.html", "15.2 Example data for statistics", " 15.2 Example data for statistics Contents to be added here. "],["single-omic-analyses.html", "16 Single omic analyses", " 16 Single omic analyses Here is a review of existing methods. "],["data-transformations.html", "17 Data transformations", " 17 Data transformations Here is a review of existing methods. "],["unsupervised-exploration.html", "18 Unsupervised exploration", " 18 Unsupervised exploration Unsupervised methods include exploratory techniques, such as cluster analysis and ordination-based visualisation methods, which reveal the structure and main patterns of the omic datasets without prior information about experimental design. These procedures might reveal that the observations are structured into meaningful groups or that variables can be reduced to fewer dimensions. Researchers can then opt for using the outputs of these analyses, rather than the original multidimensional datasets, for the multi-omic data integration. Most clustering and ordination techniques are computed from association matrices, thus it is essential to do an appropriate pre-transformation of the data and choice of association coefficient, since this influences the final outcome of the analyses Cluster analysis Dimension reduction and ordination "],["cluster-analysis.html", "18.1 Cluster analysis", " 18.1 Cluster analysis Clustering procedures group features or observations into homogeneous sets by minimising within-group and maximising among-group distances 18.1.1 Hierarchical clustering Hierarchical clustering produces a stratified organisation of features or observations where relatively similar objects are grouped together. The clustering can be performed using different criteria to measure the distance between clusters, which will affect the final outcome of the analysis (e.g., single linkage, complete linkage, average linkage and Ward’s minimum variance). #Example code goes here A useful exploratory analysis to reveal general patterns in an omic layer can be obtained by simultaneous application of hierarchical clustering to the rows and columns of the data matrix, and visualising the results in a heatmap. #Example code goes here 18.1.2 Disjoint clustering Disjoint clustering techniques aim at separating the objects into individual, usually mutually exclusive, and in most cases, unconnected clusters. K-means clustering is one of the most typical algorithms where objects are assigned to k clusters using an iterative procedure that minimises the within-clusters sums of squares. Other available clustering methods include twinspan, self-organising maps, dbscan and Dirichlet multinomial mixtures (DMM). DMM were specifically developed to analyse MG data but can be equally useful for other sequencing-based omic datasets. #Example code goes here "],["dimension-reduction-ordination.html", "18.2 Dimension reduction and ordination", " 18.2 Dimension reduction and ordination Ordination is a method complementary to data clustering, which enables displaying differences among samples graphically through reducing the dimensions of the original data set, so that similar objects are near and dissimilar objects are farther from each other. 18.2.1 Principal Component Analysis (PCA) Principal component analysis (PCA) is one of the most widely applied methods for ordination. PCA generates new synthetic variables (principal components) that are linear combinations of the original variables and capture as much variance of the original data as possible. The principal components are orthogonal to each other and correspond to the successive dimensions of maximum variance of the scatter of points. The distance preserved among objects is euclidean and the relationships among variables are linear, thus PCA should generally be applied after appropriate transformations. #Example code goes here 18.2.2 Principal Coordinate Analysis (PCoA) Content to be added here. #Example code goes here 18.2.3 Non-metric Multidimensional Scaling (NMDS) Content to be added here. #Example code goes here 18.2.4 t-Distributed Stochastic Neighbour Embedding (t-SNE) Content to be added here. Requires many data points. #Example code goes here 18.2.5 Uniform manifold approximation and projection (UMAP) Content to be added here. #Example code goes here 18.2.6 Potential of heat diffusion for affinity-based transition embedding (PHATE) Content to be added here. #Example code goes here "],["supervised-analysis.html", "19 Supervised analysis", " 19 Supervised analysis The supervised analyses of omic layers, in contrast to unsupervised ones, incorporate information of experimental design, and can be divided into two types of problems: regression and classification. A regression problem is when the output of the model is a numeric variable or a matrix, such as the phenotypic characteristics of the host or the omic data sets themselves. These methods aim at testing and estimating the effects of the experimental factors (e.g., dietary treatment, drug administration) or variables of interest (e.g., age of the experimental subjects, geographic location of studied populations) on different omic layers, or associating the omic layers with host phenotypic features. A classification problem is when the output of the model is categorical. In the context of multi omic studies, classification methods aim at classifying observations into their experimental groups (e.g. health status, dietary treatment) based on their features on different omic layers. "],["regression-methods.html", "19.1 Regression methods", " 19.1 Regression methods Independently testing the effects of the experimental factors of interest on different omic layers can be very informative to get an overall picture of how the host and the microbiome are responding to the environment and/or the experimental treatment. 19.1.1 PERMANOVA Content to be added here. #Example code goes here 19.1.2 ANOSIM Content to be added here. #Example code goes here 19.1.3 Redundancy analysis (RDA) Content to be added here. #Example code goes here 19.1.4 Canonical Correspondence Analysis (CCA) Content to be added here. #Example code goes here 19.1.5 Generalised linear modelling (GLM) Content to be added here. #Example code goes here 19.1.6 Generalised linear mixed modelling (GLMM) Content to be added here. #Example code goes here "],["classification-methods.html", "19.2 Classification methods", " 19.2 Classification methods It is in classification problems where ML algorithms have proven most useful. 19.2.1 Random Forests (RF) Content to be added here. #Example code goes here 19.2.2 Support Vector Machines (SVM) Content to be added here. #Example code goes here "],["multi-omic-integration.html", "20 Multi-omic integration", " 20 Multi-omic integration Here is a review of existing methods. "],["multi-staged-integration.html", "21 Multi-staged integration", " 21 Multi-staged integration Here is a review of existing methods. "],["meta-dimensional-integration.html", "22 Meta-dimensional integration", " 22 Meta-dimensional integration Here is a review of existing methods. "],["useful-links.html", "23 Useful links", " 23 Useful links Genomics Data Wrangling and Processing for Genomics (website): Shell command line usage Introduction to the Command Line for Genomics (website): general overview of basic command line usage. R usage (General usage and programming) Intro to R and RStudio for Genomics (website): Efficient R programming (website): best practices for programming in R. R usage (Graphics and visualisation) Fundamentals of Data Visualization (website): guide to making visualisations that accurately reflect the data, tell a story, and look professional. R Graphics Cookbook (website): a practical guide that provides more than 150 recipes to generate high-quality graphs using ggplot2. Statistics An Introduction to Statistical Learning (book): freely available book about general statistical learning covering regression and classification problems through linear modelling and machine learning. High dimensional statistics with R (website): virtual lesson specialised in dealing with high dimensional data. "],["references.html", "24 References", " 24 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
