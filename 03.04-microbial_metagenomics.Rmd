# Microbial metagenomics (MG) data processing {#microbial-metagenomics-data-processing}

Microbial metagenomic data processing can be conducted following different strategies. Decision on which approach to use should be based on the aims of the study, available reference data, amount of generated data, and many other criteria. In this workbook we consider three main approaches that require different bioinformatic pipelines to be implemented.

* **[Reference-based approach](#reference-based):** it relies on a reference database of microbial genomes to which sequencing reads can be mapped to obtain estimations of relative proportion of reads belonging to each of the genomes available in the reference database. It is the simplest and computationally less expensive approach, yet it completely relies on a complete and representative reference database.
* **[Assembly-based approach](#assembly-based):** it is based on assembling sequencing reads into longer DNA sequences known as contigs, which can then be used to predict genes and perform functional analyses. The main limitation of this approach is that the entire metagenome (set of contigs) in each sample is considered as a single unit, thus overlooking which bacterial genome each detected gene belongs to.
* **[Genome-resolved approach](#genome-resolved):** it is the most advanced of the three approaches, and the strategy that provides the largest amount of information, as the aim of this approach is to directly reconstruct all the genomes in a metagenome. This is achieved by binning contigs into Metagenome-Assembled Genomes (MAGs), which can then be taxonomically and functionally annotated to perform sound community-level analyses.

:::: {.authorbox}
Contents of this section were created by [Antton Alberdi](#antton-alberdi).  
:::

## Reference-based {#reference-based}

Reference-based metagenomics is the approach that aims at characterising metagenomes based on existing genome sequences that are used as reference. This approach is meaningful when the microbial community  under study is well known, such as the human microbiome, or when a reference MAG catalogue has been generated from a subset of samples under analysis. When the genome catalogue used as a reference has not been generated from the same environment (e.g., using human gut microorganisms as reference for vulture gut microbiomes), there are two major risks. The first one is that some of the microorganisms present in the studied environment might not be represented in the reference catalogue, which results in diversity underestimations. The second one is that the microorganisms present in the reference catalogue are similar but not identical to the ones in the target sample, which might result in incorrect taxonomic and functional inferences. It is therefore important to keep these caveats in mind when performing reference-based metagenomic analyses, mainly when dealing with non-model host organisms.

## Assembly-based {#assembly-based}

Assembly-based approaches for processing metagenomic data are based on assembling sequencing reads into longer DNA sequences known as contigs, which can then be used to predict genes and perform functional analyses. The main limitation of this approach is that the entire metagenome (set of contigs) in each sample is considered as a single unit, thus overlooking which bacterial genome each detected gene belongs to. Assembly-based approaches can be divided in two main strategies:

* [Individual assembly-based](#individual-assembly-based)
* [Coassembly-based](#coassembly-based)

### Individual assembly-based {- #individual-assembly-based}

Two of the most popular metagenome assemblers are **Megahit** and **MetaSpades**. Metaspades is considered superior in terms of assembly quality, yet memory requirements are much larger than those of Megahit. Thus, one of the most relevant criteria to choose the assembler to be employed is the balance between amount of data and available memory. Another minor, yet relevant difference between both assemblers is that Megahit allows removing contings below a certain size, while MetaSpades needs to be piped with another software (e.g. bbmap) to get rid off barely informative yet often abundant short contigs.

#### Individual assembly using Megahit {-}

```{sh eval=FALSE}
megahit \
    -t {threads} \
    --verbose \
    --min-contig-len 1500 \
    -1 {input.r1} -2 {input.r2} \
    -o {params.workdir}
    2> {log}
```

#### Individual assembly using MetaSpades {-}

```{sh eval=FALSE}
metaspades.py \
    -t {threads} \
    -k 21,33,55,77,99 \
    -1 {input.r1} -2 {input.r2} \
    -o {params.workdir}
    2> {log}

# Remove contigs shorter than 1,500 bp using bbmap
reformat.sh \
    in={params.workdir}/scaffolds.fasta \
    out={output.assembly} \
    minlength=1500
```

#### Assembly statistics using Quast {-}

The metagenome assemblies can have very different properties depending on the amount of data used for the assembly, the complexity of the microbial community, and other biological and technical aspects. It is therefore convenient to obtain some general statistics of the assemblies to decide whether they look meaningful to continue with downstream analyses. This can be easily done using the software **Quast**.

```{sh eval=FALSE}
quast \
    -o {output.report} \
    --threads {threads} \
    {input.assembly}
```

### Coassembly-based {- #coassembly-based}

Coassembly is the process of assembling input files consisting of reads from multiple samples, as opposed to performing an independent assembly for each sample, where the input would only include reads from that particular sample. Coassembly has several advantages, such as increased read depth, simplified comparison across samples by utilizing a single reference assembly for all, and frequently, a better capability to recover genomes from metagenomes by obtaining differential coverage information. However, it can also limit the capacity to recover strain-level variation.

Coassembling multiple samples does not require special assemblers, but only preparing the input files in the correct way to enable assemblers to perform the assembly over multiple samples. An example for metaspades is shown below:

```{sh eval=FALSE}
#Concatenate input reads into a single big input file
cat {input.reads}/*_1.fq.gz > {params.r1_cat}
cat {input.reads}/*_2.fq.gz > {params.r2_cat}

# Run metaspades
metaspades.py \
    -t {threads} \
    -k 21,33,55,77,99 \
    -1 {params.r1_cat} -2 {params.r2_cat} \
    -o {params.workdir}
    2> {log}

# Remove contigs shorter than 1,500 bp using bbmap
reformat.sh \
    in={params.workdir}/scaffolds.fasta \
    out={output.Coassembly} \
    minlength=1500
```

Note that the genome-resolved metagenomic approach also relies on assemblies or co-assemblies, but downstream binning procedures are explained in the **[genome-resolved approach](#genome-resolved)** section.

### Gene annotation {- #assembly-gene-annotation}

Gene annotation refers to the process of identifing and assigning function to genes present in an assembly. In the first step, protein-coding and other types of genes are identified using tools such as Prodigal based on structural information of the DNA sequences. These software also predict the protein sequences these genes are expected to yield, which are then used to assign functions by contrasting them with functionally annotated reference databases. Due to the amount of reference databases available, it is common practice to match the genes against multiple databases and yield multiple annotations per gene. Currently, multiple tools exist that perform all these procedures in a single pipeline, such as DFAST [@Tanizawa2017-uy] and DRAM [@Shaffer2020-kp]. DFAST annotates genes against the TIGRFAM and Clusters of Orthologous Groups (COG) databases, while DRAM performs the annotation using Pfam, KEGG, UniProt, CAZY and MEROPS databases.

```{sh eval=FALSE}
DRAM.py annotate \
      -i {input.assembly} \
      -o {outdir} \
      --threads {threads} \
      --min_contig_size 1500
```

The procedure for annotating MAGs, which is explained in the **[genome-resolved approach](#genome-resolved)** section, is identical to this one, with the only difference that the a MAG or a set of MAGs are used as input data rather than a metagenomic assembly.

### Read mapping {- #assembly-read-mapping}

The aim of assembly-based analyses is often to obtain gene-abundance information per studied sample, to characterise the functional properties of the entire metagenome as a whole (as opposed to the **[genome-resolved approach](#genome-resolved)** approach, in which functional attributes are assigned to each MAG). This requires reads from each sample to be mapped against the sequence of all protein-coding genes identified during the annotation process. All gene prediction software and annotation pipelines produce a FASTA file only containing gene sequences, which is used as the reference material.

The gene catalogue needs to be indexed before the mapping.
```{sh eval=FALSE}
bowtie2-build \
      --large-index \
      --threads {threads} \
       {all_genes}.fa.gz
```

Then, the following step needs to be iterated for each sample, yielding a BAM mapping file for each sample.
```{sh eval=FALSE}
bowtie2 \
      --time \
      --threads {threads} \
      -x {all_genes} \
      -1 {input.r1} \
      -2 {input.r2} \
      | samtools sort -@ {threads} -o {output}
```

Or relative abundance per gene per sample.
```{sh eval=FALSE}
coverm genome \
      -b {params.BAMs}/*.bam \
      -s ^ \
      -m relative_abundance \
      -t {threads} \
      --min-covered-fraction 0 \
      > {output.mapping_rate}
```

:::: {.authorbox}
Contents of this section were created by [Antton Alberdi](#antton-alberdi).  
:::

## Genome-resolved {#genome-resolved}

Genome-resolved metagenomics aims at recovering near-complete bacterial genomes from metagenomic mixtures. It relies on the assembling and read-mapping procedures explained in **[assembly-based approach](#assembly-based)** section, which is followed by a binning procedure to produce the so-called Metagenome-Assembled Genomes (MAGs).

Note that entire suites and pipelines are available for conducting all the steps outlined in this section, and often more. Some of them include:

* Anvi'o
* metaWRAP
* ATLAS

### Binning {- #genome-resolved-binning}

Metagenomic binning is the bioinformatic process that attempts to group metagenomic sequences by their organism of origin {Goussarov}. In practice, what binning does is to cluster contigs of a metagenomic assembly into putative bacterial genomes. In the last decade over a dozen of binning algorithms have been released, each relying on different structural and mathematical properties of the input data.

Two of the most relevant structural properties to group contigs into bins are oligonucleotide composition of contigs and present of universally conserved genes in contigs. MaxBin, for example, relies on such universally conserved genes to initialize clusters, which are then expanded using the oligonucleotide composition of contigs. Besides structural attributes of contigs, the main quantitative measure used for binning is differential coverage, which is computed by counting the number of reads from different samples mapped to the assembly. This information is used by binning algorithms CONCOCT and MetaBat, for example.

Metabat and Maxbin require a depth file to be generated first.
```{sh eval=FALSE}
jgi_summarize_bam_contig_depths \
    --outputDepth {output.depth} \
    {input.assemblybampath}
```

Example code for launching metabat2.
```{sh eval=FALSE}
metabat2 \
    -i {input.assemblypath} \
    -a {input.depth} \
    -o {output.basepath} \
    -m 1500 \
    -t {threads} \
    --unbinned
```

Example code for launching MaxBin.
```{sh eval=FALSE}
run_MaxBin.pl \
    -contig {input.assemblypath} \
    -abund {input.depth} \
    -out {output.basepath} \
    -thread {threads}
```

### Bin refinement {- #genome-resolved-refinement}

The performance of the binning algorithms is largely dependent on the specific properties of each sample. A software that performs very well with a given sample can be easily outcompeted by another one in the next sample. In consequence, many researchers opt for ensemble approaches whereby assemblies are binned using multiple algorithms, followed by a refinement step that merges all generated information to yield consensus bins. This final step is ofter referred to as "bin refinement", and can be performed using tools like metaWRAP [@Uritskiy2018-my] or Dastool [@Sieber2018-fp]. Several benchmarking studies have shown that such ensemble approaches are usually better than individual binning tools.

The following code can be used to run an ensemble binning using metaWRAP.
```{sh eval=FALSE}
metawrap binning -o {params.outdir} \
    -t {threads} \
    -m {params.memory} \
    -a {params.assembly} \
    -l 1500 \
    --metabat2 \
    --maxbin2 \
    --concoct \
```

The following code can be used to refine binds using metaWRAP.
```{sh eval=FALSE}
metawrap bin_refinement \
    -m {params.memory} \
    -t {threads} \
    -o {params.outdir} \
    -A {params.concoct} \
    -B {params.maxbin2} \
    -C {params.metabat2} \
    -c 70 \
    -x 10
```

### Bin quality assessment {- #genome-resolved-qc}

Metagenomic binning is a powerful yet complex procedure that yields many bins that do not properly represent bacterial genomes. It is therefore essential to assess the quality of those bins before considering them representative of bacterial genomes. The two main parameters used for bin assessment are completeness and contamination. Completeness refers to the fraction of a given bacterial genome estimated to be represented in the bin, while contamination refers to the proportion of the bin estimated to belong to a different genome. The most commonly employed software to assess bin quality is CheckM, which yields completeness and contamination metrics based on single-copy core genes.

Based on completeness and contamination metrics, a group of experts proposed some community standards to classify bins according to their quality and establish minimum quality requirements for considering a bin as a MAG [@Bowers2017-kj].

### Bin curation {- #genome-resolved-curation}

Contamination is an issue that in certain cases can be minimised by curating bins. The Anvi'o suite [@Eren2015-tt] provides a powerful visual interface to manually curate bins by dropping contigs that display distinct features (e.g., taxonomic annotation, coverage, GC%) to the rest of the contigs included in a bin. GUNC provides a way to implement a similar curation step in a more automatised manner [@Orakov2021-pt].

### Dereplication {- #genome-resolved-dereplication}

Dereplication is the reduction of a set of MAGs based on high sequence similarity between them [@Evans2020-hs]. Although this step is neither essential nor meaningful in certain cases (e.g., when studying straing-level variation or pangenomes), in most cases it contributes to overcome issues such as excessive computational demans, inflated diversity or inspecific read mapping. If the catalogue of MAGs used to map sequencing reads to (see [read mapping](#genome-resolved-mapping) section below) contains many similar genomes, read mapping results in multiple high-quality alignments. Depending on the software used and parameters chosen, this leads to sequencing reads either being randomly distributed across the redundant genomes or being reported at all redundant locations. This can bias quantitative estimations of relative representation of each MAG in a given metagenomic sample.

Dereplication is based on pairwise comparisons of average nucleotide identity (ANI) between MAGs. This implies that the number of comparisons scales quadratically with an increasing amount of MAGs, which requires for efficient strategies to perform dereplication in a cost-efficient way. A popular tool used for dereplicating MAGs is dRep [@Olm2017-nx], which combines the fast yet innacurate algorithm MASH with the slow but accurate gANI computation to yield a fast and accurate estimation of ANIs between MAGs. An optimal threshold that balances between retaining genome diversity while minimising cross-mapping issues has been found to be 98% ANI.

### Taxonomic annotation {- #genome-resolved-taxonomy}

Although not necessary for conducting most of the downstream analyses, taxonomic annotation of MAGs is an important step to provide context, improve comparability and facilitate result interpretation in holo-omic studies. MAGs can be taxonomically annotated using different algorithms and reference databases, but the  Genome Taxonomy Database (GTDB) [@Parks2022-zl] and associated taxonomic classification toolkit (GTDB-Tk) [@Chaumeil2022-jr] have become the preferred option for many researchers.

### Functional annotation {- #genome-resolved-function}

Functional annotation refers to the process of identifying putative functions of genes present in MAGs based on information available in reference databases. As explained in the **[assembly-based approach](#assembly-based)**, the first step is to predict genes in the MAGs (unless these are available from the assembly), followed by functional annotation by matching the protein sequences predicted from the genes with reference databases. Currently, multiple tools exist that perform all these procedures in a single pipeline, such as DFAST [@Tanizawa2017-uy] and DRAM [@Shaffer2020-kp]. DFAST annotates genes against the TIGRFAM and Clusters of Orthologous Groups (COG) databases, while DRAM performs the annotation using Pfam, KEGG, UniProt, CAZY and MEROPS databases.

```{sh eval=FALSE}
DRAM.py annotate \
      -i {input.MAG} \
      -o {outdir} \
      --threads {threads} \
      --min_contig_size 1500
```

These functional annotations can be used for performing functional gene enrichment analyses, distilling them into genome-inferred functional traits, and many other downstrean operations explained in the [statistics part](#about-statistics).

### Read mapping {- #genome-resolved-mapping}

When the objective of a genome-resolved metagenomic analysis is to reconstruct and analyse a microbiome, researchers usually require relative abundance information to measure how abundant or rare each bacteria was in the analysed sample. In order to achieve this, it is necessary to map the reads of each sample back to the MAG catalogue and retrieve mapping statistics. The procedure is identical to that explained in the [assembly read-mapping](#assembly-read-mapping) section, yet using the MAG catalogue as a reference database rather than the metagenomic assembly. This procedure usually happens in two steps. In the first step, reads are mapped to the MAG catalogue to generate BAM or CRAM mapping files. In the second step, these mapping files are used to extract quantitative read-abundance information in the form of a table in which the amount of reads mapped to each MAG in each sample is displayed.

First, all MAGs need to be concatenated into a single file, which will become the reference MAG catalogue or database.
```{sh eval=FALSE}
cat {MAG.path}/*.fa.gz > {all_MAGs}.fa.gz
```

The MAG catalogue needs to be indexed before the mapping.
```{sh eval=FALSE}
bowtie2-build \
      --large-index \
      --threads {threads} \
       {all_MAGs}.fa.gz
```

Then, the following step needs to be iterated for each sample, yielding a BAM mapping file for each sample.
```{sh eval=FALSE}
bowtie2 \
      --time \
      --threads {threads} \
      -x {all_MAGs} \
      -1 {input.r1} \
      -2 {input.r2} \
      | samtools sort -@ {threads} -o {output}
```

Finally, CoverM can be used to extract the required stats, such as covered fraction per MAG per sample.
```{sh eval=FALSE}
coverm genome \
      -b {input} \
      -s ^ \
      -m count covered_fraction length \
      -t {threads} \
      --min-covered-fraction 0 \
      > {output.count_table}
```

Or relative abundance per MAG per sample.
```{sh eval=FALSE}
coverm genome \
      -b {params.BAMs}/*.bam \
      -s ^ \
      -m relative_abundance \
      -t {threads} \
      --min-covered-fraction 0 \
      > {output.mapping_rate}
```

:::: {.authorbox}
Contents of this section were created by [Antton Alberdi](#antton-alberdi).  
:::
